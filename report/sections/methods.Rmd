##Methods
_________

###Ordinary Least Square (OLS)
Previous, we have predominantly used OLS to estimate the coefficients for a linear model, and the goal is to find the set of coefficients that will minimize the sum of the squares of the predicted values and the actual values of the dependent variable. The formula we want to minimize can be express as follows:

$$\sum_{i = 1}^{n}(y_{i} - \beta_{0} - \sum_{j = 1}^{p}\beta_{j}x_{ij})^{2}$$

Since this method is fundamental and unbiased, we will use this model as a benchmark to evaluate the other methods.

###Shrinkage Methods

Shrinkage methods are used to mainly constrain the coefficients estimates of the model. By shrinking the coefficients and making them smaller, the model could reduce the variance in the prediction, and as a consequence, increase the bias. We will use _Ridge Regression_ and _Lasso Regression_ to see whether shrinkage will help improve our predictions.

####Ridge Regression (RR)

Ridge regression is an example of shrinkage method applied to least squares regression. The formula we want to minimize can be express as follows:

$$\sum_{i = 1}^{n}(y_{i} - \beta_{0} - \sum_{j = 1}^{p}\beta_{j}x_{ij})^{2} + \lambda \sum_{j=1}^p \beta_j^2$$

where $\lambda$ is a positive parameter controlling the effect of shrinkage. Higher value means there is a higher penalty for large coefficients and vice versa. For example, when $\lambda$ = 0, there is no penalty on the coefficients, therefore it is regular OLS. As $\lambda$ increases, the coefficients get closer to 0. Since it is a parameter, we will use cross-validation to tune an optimal value for $\lambda$.


###Dimension Reduction Methods

Dimension Reduction method uses OLS to fit the coefficients. However, instead of using the original predictors, it will first create a new vector of predictors from linear combinations of the original predictors. This effectively reduces the number of variables we are using as predictors in our regression. There are different ways to achieve the dimension reduction, and here we will discuss _Principal Components Regression_ and _Partial Least Squares Regression_. 

####Principal Components Regression (PCR)

Principal Components Regression uses Principal Components Analysis (PCA) as a dimension reduction step prior to linear regression. Benefits of using PCA include being able to identify structures and relations, reducing overfit, and lowering dimensions. PCA transforms a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called _principal components_. The first principal component tries to account as much variability in the data as possible, and all subsequent principal components have the highest variance possible given the constraint that they must be orthongonal to the preceding components. As a result, PCA produces a new set of predictors with size _less than or equal_ to the original set, denoted as variable _M_, meaning that a smaller amount of predictors is enough to explain most of the variation in the data.

By using PCR method, we will get better results than fitting using OLS because our dimensions will most likely be lower due to the dimension reduction. With lower dimension, we can lower the risk of overfitting our model. Since _M_ will be the parameter of PCR, we will again use cross-validation to tune an optimal value for _M_. 